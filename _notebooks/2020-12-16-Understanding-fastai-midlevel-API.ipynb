{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding fastai's Midlevel API\n",
    "> \"How to customize your data processing. Exploring Transforms, Pipelines, TfmdLists, Datasets\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [fastai, data processing]\n",
    "- image: images/articles/2020-12-16-midlevel-api/datasets-title.png\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# !pip install fastai -Uqq\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after going through the course and book, I didn't feel comfortable with the numerous classes and methods that fastai offers for data processing. I felt that it would be easy once I understood it, but it just didn't *click* right away. So here I try to present the most important classes and why they are useful.\n",
    "\n",
    "This is by no means a complete overview, but just an exploration with minimal examples.\n",
    "\n",
    "For a deeper dive, please check out\n",
    "[chapter 11 of fastbook](https://github.com/fastai/fastbook/blob/master/11_midlevel_data.ipynb),\n",
    "Wayde Gilliam's [awesome blog post](https://ohmeow.com/posts/2020/04/11/finding-datablock-nirvana-part-1.html),\n",
    "or Zach Mueller's [walk with fastai2](https://www.youtube.com/watch?v=bw4PRyxa-y4)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "\n",
    "![](../images/articles/2020-12-16-midlevel-api/transforms.png)\n",
    "\n",
    "A Transform in general is an *object that can be called* and has an optional `setup` method that initializes an inner state, and an optional `decode` method that will reverse the function.\n",
    "\n",
    "> Note: `Transforms` are usually applied on tuples.\n",
    "\n",
    "In general, our data is always in tuples of `(input,target)`, although you can have more than one input or one target. When applying a transform, it should be applied to the elements of the tuple separately.\n",
    "\n",
    "When you only want to implement the encoding behaviour of a transform, it can be defined via the `@Transform` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, dear reader'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@Transform\n",
    "def lowercase(x:str):\n",
    "    return str.lower(x)\n",
    "\n",
    "lowercase(\"Hello, Dear Reader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type dispatch\n",
    "Transforms can be defined so they apply only to certain types. This concept is called *type dispatch* and is provided by [fastcore](https://fastcore.fast.ai/dispatch.html), which I'll cover in a future post :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 3.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@Transform\n",
    "def square(x:int):\n",
    "    return x**2\n",
    "\n",
    "square((3, 3.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the type annotation for `x`. In this case, this is not merely a helpful annotation like in pure Python, but it actually changes the behaviour of the function! The `square` transform is only applied to elements of the type `int`. When we don't define the type, a transform is applied on all types.\n",
    "\n",
    "If you want to also define the `setup` and `decode` methods, you can inherit from `Transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DumbTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        vocab = set([char for text in items for char in text])\n",
    "        self.c2i = {char: i for i, char in enumerate(vocab)}\n",
    "        self.i2c = {i: char for i, char in enumerate(vocab)}\n",
    "    def encodes(self, x): \n",
    "        return [self.c2i.get(char, 999) for char in x]\n",
    "    def decodes(self, x):\n",
    "        return ''.join([self.i2c.get(n, ' ? ') for n in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 6, 2, 2, 4, 999]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\"Hello\", \"Bonjour\", \"Guten Tag\", \"Konnichiwa\"]\n",
    "tokenizer = DumbTokenizer()\n",
    "tokenizer.setup(texts)\n",
    "encoded = tokenizer(\"Hello!\")\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is a representation that a machine learning model can work with. But we humans can't read it anymore. To display the data and be able to analyze it, call `decode` on the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello ? '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we defined a (very dumb) tokenizer. The `setups` method receives a bunch of items that we pass in. It creates a vocabulary of all characters that appear in `items`. In `encodes` we transform each character to a number in an index. When the character is not found in the vocabulary, it is replaced with an `999` token. `decodes` reverses this transform as good as it can.\n",
    "\n",
    "By the way: you might have noticed that in the `DumbTokenizer` class we defined the methods `setups`, `encodes` and `decodes`, but on the instance `tokenizer` we call methods with slightly different names (`setup`, `decode`) or even the instance directly: `tokenizer(...)`. The reason for this is that fastai applies some magic in the background, the book says that the `setup` method does some additional work before and after calling `setups`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "![](../images/articles/2020-12-16-midlevel-api/pipeline.png)\n",
    "\n",
    "A `Pipeline` is just an easy way to apply a list of transforms, in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 6, 2, 2, 4, 11, 5, 4, 0, 2, 999, 999]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfms = Pipeline([lowercase, tokenizer])\n",
    "encoded = tfms(\"Hello World!\")\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pipeline` also supports decoding of an item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello worl ?  ? '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfms.decode(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we didn't define a `decodes` method for `lowercase`, this transform cannot be reversed. The decoded result consists only of lowercase letters.\n",
    "\n",
    "Notice the missing letter `d` in the decoded representation. Since there was no `d` in the initial texts, the tokenizer replaced it with the token for \"unkown\", 999.\n",
    "\n",
    "What Pipeline doesn't provide is support for the setup of the transforms. When you want to apply a pipeline of transforms on a list of data, `TfmdLists` comes to the rescue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfmdLists\n",
    "\n",
    "![](../images/articles/2020-12-16-midlevel-api/tfmdlists.png)\n",
    "\n",
    "At first, your data is usually a set of raw items (like filenames or rows in a dataframe) to which you want to apply some transforms. To combine your *pipeline of transforms* with your set of *raw items*, use `TfmdLists`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Hello\", \"Bonjour\", \"Guten Tag\", \"Konnichiwa\"]\n",
    "tls = TfmdLists(texts, [DumbTokenizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initialized, TfmdLists will call the `setup` method of each Transform in order, providing it with all the items of the previous Transform. To get the result of the pipeline on any raw element, just index into the `TfmdLists`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 4, 14, 15, 4, 19, 0]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tls[1]\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tls.decode(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation sets\n",
    "The reason that `TfmdLists` is named with an `s` (lists in plural) is that it can handle a training and a validation set. Use the `splits` argument to pass\n",
    " - the indices of the elements that should be in the training set\n",
    " - the indices of the elements that should be in the validation set.\n",
    " \n",
    "We will just do this by hand in our toy example. The training set will be \"Hello\" and \"Guten Tag\", the other two go in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Hello\", \"Bonjour\", \"Guten Tag\", \"Konnichiwa\"]\n",
    "splits = [[0,2],[1,3]]\n",
    "tls = TfmdLists(texts, [lowercase, DumbTokenizer], splits=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then access the sets through the `train` and `valid` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, 7, 1, 6, 4, 8, 1, 0, 3], 'guten tag')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tls.train[1]\n",
    "encoded, tls.decode(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at at word in the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([999, 2, 4, 999, 2, 7, 999], ' ? on ? ou ? ')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tls.valid[0]\n",
    "encoded, tls.decode(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, what happened to our \"Bonjour\" here? When `TfmdLists` automatically called the `setup` method of the transforms, it only used the items of the training set. Since there was no `b`, `j` or `r` in our training data, the tokenizer treats them as unknown characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Important: The `setup` methods of the transforms receive only the items of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't we need labels?\n",
    "\n",
    "Maybe you noticed that we haven't dealt with tuples until now. We only have transformed our input, we don't have a target yet.\n",
    "\n",
    "`TfmdLists` is useful when you built a custom `Transform` that performs all the data preprocessing and returns tuples of inputs and targets. Then you can just create a `DataLoaders` object with the `dataloaders` method.\n",
    "\n",
    "Usually however, you will have two parallel pipelines of transforms - one to convert your raw items to inputs, and one to convert raw items to targets (ie labels). To do this we can use `Datasets`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "![](../images/articles/2020-12-16-midlevel-api/datasets.png)\n",
    "\n",
    "To complete our quick tour through fastai's midlevel API, we look at the `Datasets` class. It applies two (or more) pipelines in parallel to a list of raw items and builds a tuple of the results. It performs very much like a `TfmdLists` object, in that it\n",
    " - automatically does the setup of all Transforms\n",
    " - supports training and validation sets\n",
    " - supports indexing\n",
    " \n",
    "The main difference is: When we index into it, it returns a tuple with the results of each pipeline.\n",
    "\n",
    "Let's look at how to use it. For this toy example, let's pretend we want to classify whether a text contains at least one space (that's a dumb example, I know). For this we create a little labelling function in the form of a `Transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpaceLabeller(Transform):\n",
    "    def encodes(self, x): return int(' ' in x)\n",
    "    def decodes(self, x): return bool(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([10, 14, 13, 13, 8, 2, 7, 8, 22, 0], 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tfms = [lowercase, DumbTokenizer]\n",
    "y_tfms = [SpaceLabeller]\n",
    "dsets = Datasets(texts, [x_tfms, y_tfms], splits=splits)\n",
    "item = dsets.valid[1]\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('konnichiwa', False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets.decode(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, we can create a `DataLoaders` object from the `Datasets`. To enable processing on the GPU, two small tweaks are required. First, every item has to be converted to a tensor (often this will happen earlier, as one of the transforms in the pipeline). Second, we use a fastai function called `pad_input` to make every sequence the same length, since a tensor requires regular shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dsets.dataloaders(bs=1, after_item=tensor, before_batch=pad_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [([7, 4, 11, 11, 14], 0),([6, 20, 19, 4, 13, 999, 19, 0, 6], 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now a ready-for-training dataloader!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We looked at how to customize every step of the data transformation pipeline. As mentioned in the beginning, this is not a complete description of all the features available, but a good starting point for experimentation. I hope this was helpful to you, let me know if you have questions or suggestions for improvement at <hannes@deeplearning.berlin> or [@daflowjoe](https://twitter.com/daflowjoe) on Twitter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
