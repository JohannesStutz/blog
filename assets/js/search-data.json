{
  
    
        "post0": {
            "title": "The Private AI Series, Course 1, Part 5",
            "content": "The Impact of Structured Transparency . Part 1 of my summary of the Private AI series was all about information flows and how they are fundamental to our society. We also learned about how information flows are often broken today because of the privacy-transparency trade-off. . In Part 2, we discussed which technical problems exactly are underlying the privacy-transparency trade-off. . In Part 3, we learned about solutions. Structured transparency lets us analyze privacy issues in a structured way. We learned about input and output privacy, two of the five guarantees of structured transparency. . In Part 4, we continued with structured transparency. Covering input and output verification as well as flow governance. . In Part 5, we learn about the groups who will be affected by these new techniques. We have to understand their motivations and pain points to predict how they will respond. This will prepare you to take advantage of the opportunities that are arising. . Academic Research . The primary motiviation of researchers: answering important, impactful questions. To answer these questions, researchers need access to quality information. This need for data makes collaboration between researchers important. But once a researcher shares a copy of their data, it becomes difficult to control what others do with it. The copy problem. That&#39;s why collaboration today is often very slow. . Note: The goal is to create simple workflows for maximizing research use of data, while minimizing the risk of misuse. Structured transparency tools allow better information flows. Here, it ideally would allow researchers to answer their specific questions, while the data owner maintained control over the data. This works because researchers don&#39;t necessarily need a copy of the data. They only need to know the results of their analysis, i.e., the outputs of an ML model or the average of a value. . How does each component of structured transparency apply to researchers? . Input Privacy: SMPC allows collaboration without sharing plain-text data | Output Privacy: data owners could prevent reverse-engineering of the outputs of a computation, using Differential Privacy. Larger datasets can make it easier to ensure this. | Input and output privacy will be enough for most research collaborations since there is trust between the actors. | Input Verification: If trust was lacking, input verification techniques could be used. They would allow a data owner to prove specific attributes of the dataset to the researcher. | Output Verification: might be required if there is competition between institutions. Could be used to prove that a statistical result was actually computed by the data owner using the computations requested by the researcher. | Flow Governance: allows data owners to make extra sensitive data available for appropriate research. Distributing control across third parties like funding bodies or consortiums. | . . Important: More data available will lead to an increase in pace and quality of research in all empirically driven fields. There are two reasons that researchers will adopt privacy-preserving technologies. Researchers want their research to be impactful. Impact is a key feature for grant applications. . More access to data will lead to better research results. This can increase impact in comparison with a result only from self-collected data. | Grants tend to favor projects that use cutting-edge methods. Privacy-preserving tools will be cutting edge for quite a while. | Researchers have already started adopting these tools and making them available for other researchers. One example for medical imaging applications: PriMIA. . Why is access to more data important? Because we want to create AI systems that people can trust. For this, it is important that we have representative and unbiased data available to train on. It should represent the entire spectrum of what you find in real patients. The algorithms should also be universally useful. They should be applicable to people of all genders and ethnic backgrounds. . Why can&#39;t we collect all the data in one big database? . Data ownership is problematic. You need informed consent from all patients. | By assembling into big databases you lose the option to enforce any kind of governance over this data. | It might be unsafe. Examples of big datasets that were thought to be safe but weren&#39;t: Netflix prize, NYC Taxi dataset | Industry, Research &amp; Development . Research and Development are the research departments of companies. Their motivations overlap with academic researchers, they want to answer important questions. But there are two additional motivations: . Intellectual Property. The relevant data or the ML model itself might be proprietary. | Validation. A ML model in a healthcare setting needs certification to be approved. | It&#39;s very expensive to certify healthcare AI applications. You need to verify that your application works for all kinds of patients in different settings. Specifically: different genders, age groups, ethnicities. You also need to make sure that the different devices that generate diagnostic data all work well with your algorithm. For this purpose you need access to private datasets. This is very difficult. It takes about 16 to 30 months to validate an algorithm, with costs from 1.5 to 2.5 million dollars. A huge burden for anyone developing an algorithm. AI will continue to grow in healthcare, but these kinds of costs and risks are a real obstacle. . What we need: an environment where algorithms can be validated on private datasets. Without ever moving or sharing the data. The algorithm IP has to be protected as well. If the weights of a deep learning model leaked, it would pose a great risk for the company developing the algorithm. An example for such a tool: BeeKeeper AI . There are many reasons why industry already adopts these techniques for structured transparency. In addition to all the benefits for academic researchers, using these tools can give industry researchers a competitive advantage. They can speed up regulatory processes and reduce costs. . Consumers . Consumers want better privacy. That&#39;s true, but far from the only point that can be improved by structured transparency. The goal: create information flows that maximize good in the world. . One ancient information flow that could be improved: community wisdom. Folk tales, religion and community norms are forms of community wisdom. We hold on to these ancient forms of community wisdom. But because this is such a deep human desire, new ways of exchange appear with new technologies. One example is anonymous advice on the internet. Reddit, Quora, Health advice. There are thousands of communities on the internet for this kind of information flow. . Word of mouth doesn&#39;t scale well for several reasons: . Reason 1: There are millions of products. Maybe you know no one that uses this product. . Reason 2: You might not feel comfortable asking people you know. If you wonder about a certain medication or if a particular doctor has a high reputation. . Tip: Some current information flows are held back by the need for privacy or anonymity or comfort. This is your opportunity: build something better! Using input and output privacy techniques. Reason 3: Missing trust. The key strength of word of mouth is trust. But online it&#39;s hard to trust someone. Example: Online product reviews. Without verification, it&#39;s difficult to trust a review of a product. Or look at anonymous forums: which of these strangers can you trust? . Reason 4: Wisdom sometimes is not with one single person. Wisdom can mean learning from the aggregate of our experiences. That&#39;s exactly what machine learning was built for! . Focus on the goal: help people getting trusted information. Which they need to make everyday decisions. . Remember the Polis system from lesson 2: it&#39;s not about creating the most efficient, most scalable solutions. But about replicating information flows for community wisdom that have worked organically. . News Media . News Media could also profit from structured transparency solutions. . Journalists have to protect their sources, but want to get out their message. This is an excellent example of real-world, manual structured transparency. . Input and output privacy: hiding the source and modifying the contents to prevent identification of the source. | Input verification: by testifying that a source is credible. | Output verification: journalists are subject to review by the editor. This provides governance over the information at hand. | . Which parts of this information flow could be improved? . Example: Whistleblowing. Journalists spend their careers on building a trusted network. A very manual process, with meetings and phone calls. There is a lot of stake for a whistleblower, they have to trust a journalist. It&#39;s a classic structured transparency problem. How to reveal information about the wrongdoing of an organization, without endangering yourself? Today, there are Signal or WhatsApp that end-to-end encrypt your messages. But what is missing: you can&#39;t message someone anonymously. There is no way to use existing public-key cryptography techniques to prove that you are a member of a group. If you have explosive information about an organization, it makes a difference for your credibility if you can prove that you work there. . There is also no anonymous search and discover feature. Most of this work must be done through phone calls, emails, network building. It&#39;s possible, but with a lot of friction. If there was less friction, there could be more stories and more accountability for people in positions of power. . Example: Disinformation. It&#39;s an input verification and output verification problem. With the rise of deepfakes, we cannot trust video anymore. We need better input verification. This is possible with embedded cryptography in cameras. The camera adds a cryptographic signature to the image. If you saw an image that no actual camera signed, you should be suspicious. . Note: This course should give you the means to break down information flows into subproblems. . Machine Learning Startups . The essential challenge for any machine learning startup: Do you have the data? On day one you have no customers. You need a strategy to get a dataset that is big enough. Big enough to create an ML model that can solve someone&#39;s problem. This is why nearly all ML startups start with consulting. Consulting can give you a relationship with a big corporation with millions of users and their data. . Data markets today are one of two very different kinds of markets. Either it&#39;s a race to the bottom where data is sold as fast as possible. Or it&#39;s a secretive industry where data is almost never sold. . Example: Financial data . On one hand, there is information about stock trades that is available to almost everyone. The only way to gain an advantage is to receive this information a bit earlier. That&#39;s why companies pay millions to get physically closer to the NASDAQ servers. | On the other hand, there are hedge funds who discover new data that helps predict the market. They never tell anyone about it because others would compete. | . This is true for other industries. Some data is traded very fast, some of it almost never. The marketplace for data is fundamentally broken. It&#39;s the copy problem: If you sell data, every customer becomes your competitor. Both for use and sale of this data. . What if the copy problem was solved? A creator could sell data in a way that they are always able to charge money for any derivative use. . That would raise the prices for data that was previously sold as fast as possible. | The use of data that was previously locked away now could be sold. No customer would also become a competitor. | Structured transparency solves this through a combination of all five guarantees. It&#39;s called Federated Data Networks. Instead of selling data, you sell the opportunity to run statistics on your data. If you&#39;re a hospital, you don&#39;t sell your MRI scans, but allow researchers to study the data while it&#39;s in your hospital. Only the high-level insights of the researchers leave the building. Data markets would explode in value. Everyone can participate and prices stay high. This technique is called Federated Learning. . Tip: Privacy-preserving technologies will cause a step change in opportunities just like the ML revolution. Unlock access to data: You are a company that sells insights on political opinion. Today, in most cases you are not allowed to access the relevant data. You cannot use data on political opinion without consent. Remember Cambridge Analytica. You needed a data market place where people could post their private information (political orientation) homomorphically encrypted. This allowed you to run analyses on this data. You needed consent from the users to publish aggregate data, but you would respect their privacy. . Privacy protection can benefit users and companies: There are people who are unwilling to share data because of privacy concerns. With Differential Privacy, there is a relationship between Îµ and the accuracy of your analysis. With perfect privacy - setting Îµ to 0 - you cannot learn anything about the data. But offer no privacy at all and you might not be able to collect the data. Even if you, as a data analyst, didn&#39;t care about personal privacy at all. Only about the accuracy of your analysis. You would have to choose something in between to get the optimum accuracy. . What does a healthy data market look like? It&#39;s privacy-perserving, far more profitable and much less risky for everyone involved. Startups have access to all the data in the world from day one. They would of course have to pay for it, but it&#39;s all available. Who owns the data? Whoever collected it first. Today, this is mostly large enterprises. But tomorrow, it could be individual people who own their data. . Note: Data about people starts with people. The most profitable thing for them to do is never let go of the original copy of their data. Let the market come to you. . Consumers Need Structured Transparency Institutions . The gist: There is a missing institution within the marketplace for private data. It should protect consumers from harm, and also enable a healthy data market place. . A quick recap of differential privacy and privacy budgets): privacy budget is a measure of leakage that a data owner gives to a scientist studying their data. It is measured using a metric called Îµ. It can limit the probability that harm could come to a participant in a data study. . Privacy budgets should be person-centric. A person&#39;s Îµ should be tracked across different institutions. No individual data holder can ensure that people don&#39;t get hurt. Because they don&#39;t know where else that person&#39;s data might be hosted. . We look at this problem from two perspectives, economics and public safety. . Economic Perspective . Brilliant paper: Selling Privacy at Auction. You should read at least the first few pages. The idea: If you sell data in a privacy-preserving way, you&#39;re not selling data itself but insights. But if you sell too many insights, somebody can use all these insights to recreate the original data. The privacy budget is all about preventing that from happening. By keeping track of the probability somebody could reconstruct your information. . But if you can only release so many insights before somebody can use these insights to reconstruct your data, then Îµ itself is the scarce resource. At some point, no further data science can be applied to your information. As there is less and less Îµ going around, its price should increase. . Note: Differential Privacy provides a way to quantify the cost of privacy. Economics of privacy: . In federated data networks, data owners maintain control over the only copy of their data. We measure data leakage using Îµ. | Îµ becomes an important factor in the price of a data point. | It is a measure of scarcity, how much is left of a dataset insight to sell. | People&#39;s interest to protect their privacy starts to align with the commercial incentive. Because businesses want to maintain control over the only copy of their information to keep prices high. Economic forces that used to work against privacy could soften or even work in favor of privacy. | . &nbsp; . Note: Îµ is an effective market mechanism to create healthy data markets that protect privacy. . Public Safety Perspective . The economic approach described above relies on an important piece of privacy infrastructure. It&#39;s called automatic privacy budgeting for remote data science. However, it is not yet mature. . At the moment, the tooling is limited. There are individual algorithms for Differential Privacy available. But no end-to-end infrastructure to keep track of privacy budgets. One of the essential lessons: privacy doesn&#39;t come for free. We must think of it as a limited resource. But it is necessary to consistently keep track of this privacy budget. . All this would only be useful to track individual privacy budgets. However, this isn&#39;t enough. Privacy budgets have to be person-centric to provide true protection! These systems can never be truly robust, unless someone, somewhere is keeping track of a person&#39;s Îµ across all institutions that have data about them. . Important: A public institution should keep track of your global privacy budget. . Intelligence Agencies, Statistical Services . These are examples of the recursive enforcement problem. . There are a lot of areas in government and statistical services that need better information flows. . Example: database queries. A government intelligence agency asks a private company to reveal information about a user. For example, the location history of a user. But they don&#39;t want to reveal which user they are looking for. (Because this could leak to the press and ruin the investigation.) The only way to do this without tools for structured transparency: the company has to reveal the data of all their users. This is a serious breach of privacy for all uninvolved users. They would have to trust that their information was not misused, if they even knew that their privacy was violated at all. . With tools for structured transparency it would be possible to do a homomorphically encrypted database query and fetch just the one record that you needed. . Note: Structured transparency changes what can be possible to demand. Whether you agree or disagree if private companies should reveal data to state actors at all. It makes a difference if millions of people&#39;s data were revealed when this was not necessary! . Regulators . Writing regulation is hard. Privacy requirements precede the development of mature technology. But there is active research around privacy and regulators should look at the science. . Regulations are sticky. Some current regulations (for example, HIPPA) encode privacy protections that we view as misguided today. They often focus on anonymization. But better solutions are out there and regulators should look at the science. The US census bureau actually adopted differential privacy techniques for the last census. . Important: Laws must be updated with new privacy technologies. Analogy: Privacy loss is like radiation. Privacy leakages accumulate. A little bit is okay, but a lot of it can do harm. . Current privacy laws are not written from this global perspective. Most laws focus on reducing the individual harm of any particular interaction, any particular data release. What we need is a holistic regulation that limits the amount of &quot;radiation&quot; that any consumer will ever experience. . The Next Steps for Structured Transparency . We looked at how structured transparency can solve the problems of many different groups of people. But there is one question remaining: When? Timing is important. . The current techniques are the result of decades of work. Some of those techniques are working on your phone right now. For example, federated learning to predict your next words. Millions of users switched to Signal to get better information flows. Consumer demand has already started. . What are the next steps, how can we solve the information flows from lesson 2? Remember, solving privacy would have great benefits: . Researchers can access data to solve important problems | Consumers can reclaim agency of their data and fight monopolies | Citizen-lead data projects can affect policy and protect their environment | Cross-border data governance does not lead to new colonialism | Healthy flow for democratic feedback | More accountability for powerful actors | . First step: Awareness. A shared vision. The execution of those information flows will require multi-disciplinary action. We need policy changes and activism. . Next step: Making it easy to build. Wrap up complex algorithms in easy-to-use tools. So that researchers and developers can adopt the techniques covered in this course. This is a great development opportunity and a great role for open source communities. Free open source software will ensure that any developer can have an accessible toolkit. . Another way: Extend current popular tools to have privacy-preserving abilities. SQL is not by default a privacy-preserving language. Multiple research teams are working on enhancing SQL to support DP queries. Combining a familiar language like SQL with sophisticated privacy protection. The result looks and feels like SQL, but the answers have DP baked into them by default. . Practical challenges: Algorithm speed, network connectivity, balancing the trade-offs of each technique. But these are exciting R&amp;D opportunities. . Once the complex algorithms are wrapped in easy-to-use tools, those tools can be wrapped into easy-to-use consumer products. So that you as a consumer don&#39;t have to think about it, like HTTPS. . Note: Regulation might one day require companies to adopt these technologies. But companies are already adopting these techniques today, way before it will get mandatory. Product strategy: Proprietary software is a short-term strategy for companies. But it will be replaced by free open-source software in the long run. Just like it happened with machine learning packages. We all use the same handful of open-source tools today. . The long term defensible product strategy is to provide information within the forthcoming networks of data. It&#39;s about building information flows that help people achieve their goals in the best possible way. . Opportunities: . If we want an existing technology to become mainstream: it&#39;s an opportunity for entrepreneurs, investors, activists and lawmakers. | If we want a novel technique to become mainstream: it&#39;s a research opportunity. And an opportunity for government investment. | If we want technologies that don&#39;t exist yet: that&#39;s an opportunity for science fiction authors. And for all of us to daydream. | . Conclusion . This summary concludes the first course of the Private AI series. I hope these summaries were helpful for you! . Image by Dirk Wohlrabe from Pixabay. .",
            "url": "https://deeplearning.berlin/online%20courses/privacy/the%20private%20ai%20series/openmined/2021/01/25/Our-Privacy-Opportunity-Part-5.html",
            "relUrl": "/online%20courses/privacy/the%20private%20ai%20series/openmined/2021/01/25/Our-Privacy-Opportunity-Part-5.html",
            "date": " â€¢ Jan 25, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "The Private AI Series, Course 1, Part 4",
            "content": "Continueing Structured Transparency . Part 1 of my summary of the Private AI series was all about information flows and how they are fundamental to our society. We also learned about how information flows are often broken today because of the privacy-transparency trade-off. . In Part 2, we discussed which technical problems exactly are underlying the privacy-transparency trade-off. . In Part 3, we learned about solutions. Structured transparency lets us analyze privacy issues in a structured way. We learned about input and output privacy, two of the five guarantees of structured transparency. . In Part 4, we continue with structured transparency, covering input and output verification as well as flow governance. . Input Verification . We learned about how input and output privacy can help you hiding information you don&#39;t want to share. But how do we know that information from such a well-hidden information flow is true? . Examples: . If a data scientist can&#39;t look at her data, how does she know what it looks like? | A police officer wants to check if a driver is of legal age. He would usually look at the driver&#39;s license. This comes bundled with all kinds of information: date of birth, photo, license ID, address... When all he has to know is that YES, you are allowed to drive. But how could he verify a card that only said YES? | . The driver&#39;s license is a good example of internal consistency. It makes it hard to create fake documents, videos, or money. It also makes it hard to pretend you&#39;re someone else in real life. . Note: Internal consistency refers to someone in the real world recognizing how things usually look like. Example: You receive a banknote that was printed on a regular printer. You would immediately feel that something is not right, even if the layout etc. is correct. . Internal consistency is one of humanity&#39;s most powerful tools for communicating trust. But this approach to verify information has two problems. . With enough effort, internal consistency can be faked. From classic approaches like faking dollar bills to modern, AI-based techniques called deepfakes. | It requires sending way more information than is necessary to communicate. It has data leakage built-in! | We need tools that allow verifying a piece of information. In a way that it a) can&#39;t be faked and b) does not require revealing any extra information. . Technical Tools for Input Verification . Tool 1: Cryptographic Signatures . When you look at the top left of your browser, you probably see a little padlock ðŸ”’. This symbolizes one of the most extraordinary input verification technologies ever deployed. When you enter a URL in your browser, hundreds of computers and network switches and cables help you find the right files. What if only one of these machines lied and sent you an altered version of the website? Would you notice it, if the website looked just as you expected? . This is called a man-in-the-middle attack. It is one of the most important input verification problems. . Note: A man-in-the-middle attack is when someone gets between you and another party and alters messages you are exchanging. This kind of attack is impossible when the little lock ðŸ”’ in your browser appears. It means that your communication with a server is encrypted with HTTPS. . HTTPS is an example of the most important tool for input verification: A cryptographic signature. . Note: A cryptographic signature allows someone to sign a document with a mark that only they could make. It has a significant advantage over real-life signatures. It cannot be copied! Remember public-key cryptography? The public key lets you encrypt something that only the private key can decrypt. . Cryptographic signatures are like this, too. You can sign any digital document with your private key. It does not matter where these documents go. Anyone who receives the signed document can verify with your public key that it is in fact your document. . What does it mean to &quot;sign&quot; a digital document, an image, an audio file? Signatures are just a mark. They are attached to an object, affiliating this object to you. What keeps a signature attached to this object? Digital signatures are actually a second file that you are sending with a document. We call this second file the certificate. You need the certificate to guarantee that this document is yours. . When you enter google.com in your browser - how do you know the page you receive is actually from google.com? Couldn&#39;t anyone copy the public key (it&#39;s public, after all)? This is what a certificate authority is for. It provides a big list of domain names and associated public keys, called certificates. When you receive a webpage that&#39;s supposed to be from google.com, your browser checks its signature using the public key from the registry. . How do we know that a signature is associated with this particular file, and that nobody altered the file? For this, we need to know what a hash is. . Note: A hash is a way to convert any kind of digital object into a single big number. It is deterministic, which means: no matter who calculates the hash, the same document will always result in the same hash. If you change even one letter in a text document or one pixel in an image file, the hash will be completely different! When google.com signs a website before sending it to you, this is what it actually does. It hashes the big page into a single number. When it creates a signature, it&#39;s a signature for that number. It is unique for this page! . Tip: If this was not clear or you want to gain a better understanding of cryptographic signatures, please check out this great video . Tool 2: Active Security &amp; Zero-Knowledge Proofs . From a structured transparency perspective, cryptographic signatures are only good for a single, straight pipe of information. Any alterations of the content would corrupt the message. This is good in case a hacker messed with your data. But it also prevents us from performing any useful computations along the way. What we want is a tool that allows transformation of the data, but only in a very specific way. How would input verification work in such a case? . We look at two of the many techniques looking to verify a flow and its inputs. . Zero-Knowledge Proofs | Encrypted computation (like HE or SMPC) with Active Security | Example: Your country holds an election. When you see the big number on the TV screen, counting the votes: How can you be sure that your vote was counted? Goal: We need something like a super-signature for the result. This would allow to proof that all inputs were used in the calculation of the final result. Every voter could check - with their public key - if their signed vote was included. . This is the idea behind encrypted communication with active security or zero-knowledge proofs. If a computation is agreed upon, then a signature can be created when information is processed in an information flow. There is cryptographic evidence allowing anyone who participated in the computation that their input was included in the final result. . Note: While these technologies exist, using them at the scale of an actual vote would be extremely challenging today. It is still ongoing research work. These concepts are not only applicable to simple computations like voting, but also for complex tasks like machine learning algorithms. You could have proof that your data was processed by a fair algorithm. . Public/private key pairs for groups of people . In previous lessons, we learned about private/public keys. One key pair was always linked to one person. But a key could also proof your anonymous identity. Think of it like a username: it can be anonymous, but represents the same person every time. | There can be an infinite amount of key pairs. Those could be used not only by individuals, but by groups of people. Like all doctors in your town or everyone who is a English/Spanish interpreter. These public/private key pairs could allow you to proof that you are a member of a group, without revealing which member exactly you are. | Your real-life signature is tied to your identity. In contrast, cryptographic signatures don&#39;t have to be this way. They can be used to verify only specific attributes about you. Returning to our popular bar example: you could have a private key which proofs that you are of legal age to drink. If the bartender asked you for ID, you could prove that you are a member of the group that is over a certain age. Without having to reveal all the private information on your ID card. . This is input privacy and input verification at the same time! . Question: One thing that isn&#39;t clear to me: What if the private key gets stolen or someone shares it with a person outside the group? When the group is large enough, isn&#39;t it very probable that the key would leak? Please let me know if you have any thoughts on that. . Reputation . A new concept: here we don&#39;t mean reputation not for people for companies, but the reputation for a verified input. This is a topic of active research, but it&#39;s supposed to inspire creativity. . Example: You are a data scientist and you are connected to a network of data owners. Let&#39;s say there are 635 data owners who claim to have data on breast cancer. But because it&#39;s private data, you&#39;re not allowed to actually look at this data. Even though you can&#39;t see the data, they want to charge you 10$/day to train your ML model. How do you decide which one of these 635 owners you trust? What would prevent anyone from attaching themselves to the network and pretending to have data on breast cancer? . Some of them might be entities you know and trust. Let&#39;s say 40 of the data owners were well-known hospitals. They can prove their identity via a signature and have a brand they don&#39;t want to damage, so you can trust their data. | What about the other 595 data owners? Well, if you are the first person using a private dataset, you can&#39;t know. You just have to try it. The data owner should probably let you do so for free. | Let&#39;s say you are the first person to use data owner 346&#39;s dataset on breast cancer. You train an ML model, validate it with a known dataset, and it does pretty well. 346 had some useful data! You leave a review: &quot;The dataset worked&quot;. You sign it with your private key. You could even leave cryptographic proof that you did the analysis and which accuracy you achieved. The second, third, fourth, ... person comes along and sees your review. They can try the dataset for themselves and leave a review as well. They are all co-signing the same statement: This dataset, which you cannot see, is actually good. As the dataset becomes more verified, its price should increase. . What if there hadn&#39;t been a trusted dataset to evaluate with? Wasn&#39;t it lucky that a hospital had a similar dataset you could trust and compare? The solutions to this problem are complex, but there are ways to deal with it. This category of solutions is called endorsements through a social network. . Important: You cannot invent trust out of nowhere. But you can take small amounts of trust and scale that trust to an entire community. Humanity is very connected. On LinkedIn, there is a feature that shows the degree of connection to other people. 1st degree connections are the people in your contact list. 2nd degree connections are the people your contacts have as 1st degree connections. Similar approaches could work to estimate whether an anonymous review or signature is real. You could look at the connections of this signature to people you actually know. If you can&#39;t find any connections, then it probably is fake. . This is futuristic, but it shows how much trust and collaboration we can build as these tools make progress. . Output Verification . . Note: Output verification asks: how do we verify that the outputs from a hidden information flow contain the properties we want? Output verification is one of the most important components of structured transparency. It allows you to verify that an algorithm, that&#39;s affecting your life, is actually good. . Machine learning models are often difficult to fully understand. They also can produce very biased results. . Tip: I can highly recommend Rachel Thomas&#8217; lesson on Data Ethics from fast.ai. It is a great introduction on the topic of Bias in AI. If we want to use algorithms, we need a way to test and audit an algorithm&#39;s performance. Fairness &amp; Bias in ML are an area of active research and they are very important. For ethical reasons first, but also for legal reasons. An example is the automatic processing of applications at some companies. The algorithm must now lower the chances for someone because their gender, race, etc. are historically underrepresented in the dataset. . This is a complex challenge. Let&#39;s cover some of the highlights. . Think about a world without digital tools, when all decisions were made by humans. Do we know that any particular decision was made in a fair way? If a person is following a procedure, we can analyze the procedure. But if a person simply makes a decision, it can be difficult to prove that this was an unfair decision, i.e., a gender-based decision. . How has society responded to human brains being black boxes and the bias of human judgement? We have laws. We have procedures for what people are and aren&#39;t allowed to do. A formal policy, a set of procedures someone has to follow to ensure their decisions are fair and just. It&#39;s not perfect, but it brings the decision-making process into the open. . In the long run, algorithms have the potential to be more transparent and more consistent than humans. However: most of the decisions humans make today are not ready for algorithms to take over. Algorithms are not accurate enough yet. But while the early use of algorithms has made mistakes regarding fairness and bias, Andrew is optimistic. The long-term potential for more fairness, transparency, and accountability is promising. Because algorithms are easier to audit than a person. . How to certify algorithms that we can&#39;t look at, because they&#39;re private? . Let trusted third parties or regulators audit them. Example: The Coca Cola recipe is famously secret, but someone from the government had to certify that it&#39;s safe. But there will not be a regulator for every possible case. That brings us to the second point: | Construct a new private information flow to evaluate the algorithm. Use a different algorithm to evaluate this one. Use input and output privacy techniques to analyze the algorithm. You could measure how accurate, fair, safe, ... an algorithm is, without having access to the algorithm itself. | Flow Governance . Setting up an ideal information flow is not enough. If anyone could change the guarantees, that would be a problem. Who should have permission to change the guarantees? This is regarding input/output privacy as well as input/output verification. . Important: Without proper governance mechanisms, the integrity of the other guarantees are at risk. There are different forms of governance: . Unilateral governance: An example is simple public-key encryption. I encrypt my message with your public key and send it to you. This is now an object only governed by you. Nobody else could use the information within the message. Homomorphic encryption is slightly different. If somebody sends me homomorphically encrypted information, I have some governance over it. I can&#39;t read it, but I can modify it, perform calculations on it. Nevertheless, it is controlled by only one person, the private-key holder. | Consensus governance: Remember additive secret-sharing from SMPC)? Where a number was encrypted between multiple shareholders. Someone could only perform calculations on the number or decrypt it if all shareholders decided to allow it. Each shareholder has veto power. | Threshold schemes: These allow for decisions with a democratic majority. SMPC and HE can both be configured this way. You can even set an arbitrary threshold. If a percentage of shareholders larger than this threshold wants to do something, they&#39;ll have enough cryptographic permission to do it. This can mean running a computation, decrypting an output, adding a shareholder, ... | . Why is this so powerful? All these fancy tools rely on trust. Nobody is perfectly trustworthy. But it&#39;s possible to design systems where people are more likely to work together. Groups of people, with checks and balances and control over each other. . An example where distributing governance over a number is especially useful: You want to store all your photos in the cloud. You could split the photos into two or more shares (SMPC), and give them to different cloud providers. You could still use their services but unless they all decided to collude, they couldn&#39;t use your information. . Note: These tools are still in development and might not work in every practical use case. Being able to share data in SMPC state is a genuine solution to the Recursive Enforcement Problem. Having information split between multiple people, with everyone having a veto power to prevent misuse, alleviates the recursive enforcement problem. . These tools give us incredible new options. But they aren&#39;t useful until we use them in systems that match the strenghts and weaknesses of humans. Computer scientists and social scientists will have to collaborate! . Conclusion . This lesson explored the remaining three guarantees of structured transparency: Input Verification, Output Verification, and Flow Governance. . Thank you for reading, and as always: please let me know if you found this summary helpful or if there is anything you found unclear ðŸ˜Š .",
            "url": "https://deeplearning.berlin/online%20courses/privacy/the%20private%20ai%20series/openmined/2021/01/20/Our-Privacy-Opportunity-Part-4.html",
            "relUrl": "/online%20courses/privacy/the%20private%20ai%20series/openmined/2021/01/20/Our-Privacy-Opportunity-Part-4.html",
            "date": " â€¢ Jan 20, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "The Private AI Series, Course 1, Part 3",
            "content": "Introducing Structured Transparency . Part 1 of my summary of the Private AI series was all about information flows and how they are fundamental to our society. We also learned about how information flows are often broken today because of the privacy-transparency trade-off. . In Part 2, we discussed which technical problems exactly are underlying the privacy-transparency trade-off. . In Part 3, covering the first half of lesson 4, we now learn about solutions. . The course introduces the concept of structured transparency. It aims to provide you with a structured approach to analyze privacy issues. Enable transparency without the risk of misuse. . Examples: . Secret voting is a form of structured transparency. Citizens can express their preferences without having to fear repression. | A sniffer dog. It detects whether a piece of luggage is safe, without revealing the private content. | Another current example that uses technology to allow structured transparency is arms control. Controlling nuclear warheads is very challenging. How to prove the authenticity of a warhead without revealing details about its construction? There is an ingenious method based on physics that allows just this. | . Why is structured transparency important now? . There has never been a more important time and need for structured transparency. Digital technology allows collecting and analzying sensitive data at an unprecedented scale. This introduces huge threats to privacy and social stability. | There has never been a more promising time for structured transparency. Recent developments in privacy-enhancing technologies allow high levels of structured transparency that had been impossible earlier. These technologies are improving rapidly and they are pushing the pareto frontier of the privacy-transparency trade-off. | The 5 Components of Structured Transparency . There is a large number of complex privacy technologies available. The structured transparency framework exists to help you break down an information flow into its individual challenges. Then we can figure out which technology is applicable. Structured transparency has the goal to make privacy-enhancing technologies accessible. It wants to build a bridge between technical and non-technical communities. . You should focus on the goals of structured transparency instead of single tools or technologies, because these are always changing and evolving. . The guarantees of structured transparency operate over a flow of information. . The guarantees: . Input privacy | Output privacy | Input verification | Output verification | Flow governance | Input privacy and verification are guarantees about the inputs of an information flow. | Output privacy and verification are guarantees about the outputs of an information flow. | Input and output privacy relate to information that needs to be hidden. Input and output verification refers to information that needs to be revealed in a verifiable way. | Flow governance is concerned with who is allowed to change the flow. This includes who is able to change the input and output privacy and verification guarantees. | . Input Privacy . Example: You write a letter to a friend, put it in an envelope, and give it to the postal service. The postal service then uses its knowledge and logistics to navigate the letter to the destination. All without reading your letter, thanks to the envelope. Input privacy in this case means the guarantee that the mailman won&#39;t be able to see your inputs (the contents of your letter) to your information flow. . Note: Input privacy is a guarantee that one or more people can participate in a computation, in such a way that neither party learns anything about the other party&#8217;s inputs to the computation. Let&#39;s consider a special case that is a bit surprising. What if you sent a letter to the mailman&#39;s mother, who reads it out loud to him? This would not violate input privacy, because the information flow had been completed at the moment he delivered the mail. . Important: Input privacy&#8217;s guarantee only protects the inputs to an information flow and the intermediate variables within an information flow, but not the outputs. Consider an information flow like a system of pipes, moving colored water from two inputs into one output. Colored water represents private data. Input privacy guarantees that . the pipes don&#39;t leak | information only flows one way. | This means that no person can see which color the other person pours into their pipe. . . A and B can&#39;t know which color the other person pours in.One exception: If B was connected to the output of the information flow, then they could tell by the color of the output water that someone is feeding in red water. Reverse-engineering inputs from the outputs like this is not violating input privacy. In the graphic, the grey area marks the elements of the information flow that input privacy protects. . There are non-technical solutions to input privacy. A common example is a non-disclosure agreement. . When you make sure that input privacy is satisfied, you can prevent the copy problem. Because it&#39;s impossible to copy input information you never see! . Technical Tools for Input Privacy . What&#39;s exciting about input privacy is not the guarantee itself, but the new technologies behind it. Recently proposed techniques are providing input privacy in ways that were impossible only a few years ago. . Tool 1: Public-key Cryptography . You use public-key cryptography (PKC) every day, for example while visiting this website ðŸ˜Š PKC allows to encrypt and decrypt a message using two different keys. We call these the public key and the private key. . Example: You want people to be able to send messages to you. But you want to be sure that no one intercepts these messages. For this purpose, you can use software to generate two keys. A private key that you never share with anyone. And a public key that you can share with everyone. Why can you share the public key with anyone? Because the public key has a special ability: it can encrypt a message in a way that only your private key can decrypt. . Note: In the language of structured transparency, public-key cryptography is a one-way pipe from anyone in the world to you. It doesn&#8217;t let anyone else read, change, or process a message on the way to you. This is the simplest form of input privacy. Notice that this pair of keys is only useful for messages sent to you. If you want to send messages to other people, they need a pair of keys to encrypt messages only they can read. To communicate with a group of people, everyone in the group needs a copy of the public keys of each other. . Tip: Are you using a non-encrypted messenger like Facebook Messenger or Telegram? This is a good time to switch to a secure alternative like Signal! You can compare alternatives here. . Tool 2: Homomorphic Encryption (HE) . Public-key cryptography can create a secure, one-way pipe from anyone in the world to you. However, all this pipe can do is copy information from the input of the pipe to the output of the pipe. While this is very important (for banking systems, the HTTPS protocol for web browsers, etc.), we want to expand this. What if you wanted to perform some kind of computation on this information? This is where encrypted computation comes into play. . Note: Encrypted computation means someone can compute over information, without even knowing the value of the information, because it is encrypted. Examples: spell-checking a document or translating a document from English to Spanish. Without knowing the contents of the documents! . Before 2009 this was completely impossible. The first algorithm proposing Homomorphic Encryption was too slow for practical purposes. But modern techniques allow running any kind of program over encrypted data. Even intense tasks like sorting algorithms or important machine learning algorithms have become efficient. . Important: If you run a computation on homomorphically encrypted data, the results will always be encrypted with the same key as the input data. The big generic use case: you can use cloud machines without trusting cloud providers not to look at your data. . Users truly stay in control of their information. In the language of structured transparency: The input privacy guarantee is not only present over the transfer of data, like in a straight pipe. Instead, it also covers all additional transformation or computation steps in the middle of this pipe. . Tip: As a business providing cloud services: with homomorphic encryption, you can guarantee to your customers that you cannot see their data. This is also useful for storing information. Take your bank as an example: if they stored your information with homomorphic encryption, they would not know how much money you had. But you could still deposit and withdraw as usual. . HE is very general-purpose. Multiple people can take some information, encrypt it with a public key, and then run arbitrary programs on it. The outputs can only be decrypted by whoever has the corresponding private key. . Tool 3: Secure Multi-Party Computation (SMPC) . . Note: Secure multi-party computation: Any algorithm wherein multiple people can calculate the outputs of a function, while keeping their inputs secret from each other. It&#8217;s a formal group of computational algorithms which satisfy input privacy. We take a closer look at an interesting one of those algorithms: Additive Secret Sharing. It is very powerful because it allows multiple people to share ownership of a number. This solves the copy problem! . Let&#39;s look at an example. I have the number 5 which I want to encrypt. I have two friends, Alex and Bob. I can take the 5 and divide it into two shares, for example, a -1 and a 6. Notice that the sum of these shares still is 5. But the shares themselves contain no information about the encrypted number they represent. We could as well have split it into 582 and -577. The numbers are random, but the relationship between them stores the number 5. . When I give Alex and Bob one of my two shares -1 and 6, neither of them will know which number is encrypted between them. The copy problem no longer applies! Neither Bob nor Alex could copy out the number 5 because they don&#39;t know they store the number 5. . With SMPC, no one can decrypt the number alone. All shareholders must agree that the number should be decrypted. It requires a 100% consensus. This is technically enforced shared governance over a number. Not enforced by some law, but by the maths under the hood. . What is even more amazing: while this number is encrypted between Alex and Bob, they can perform calculations on it. Let&#39;s double our number. The shares -1 and 6 would be transformed into -2 and 12. The sum of them would be 10, so 5*2âœ… . The kicker: all programs are numerical operations. At the lowest level every text document, image file or video is just a large number. We could take each individual number and encrypt it across multiple people like Alex and Bob. A file, a program, or an entire operating system could be encrypted this way. . For another example, take a look at this video. There, SMPC is used to calculate the average salary of three employees, and they don&#39;t have to reveal their individual salaries. . HE and SMPC are only two of many algorithms available for input privacy. Why do we need so many versions? The reason is: different algorithms run faster in different environments. SMPC requires high bandwidth to send the encrypted numbers between devices. But it needs only little computing power. HE on the other hand does not need high bandwidth, but it has to perform a lot of additional steps to perform calculations on encrypted data. So the optimal algorithm depends on your use case. . Output Privacy . Even if an information flow perfectly satisfies input privacy, it can still leak information inside of its contents. While input privacy is mainly concerned with the copy problem, output privacy is mainly concerned with the bundling problem. . Note: Output privacy is about ensuring that certain subsets of information do not make it through the information flow. If I have an information flow with inputs and outputs, how much can I reverse engineer about the inputs by reading the outputs? . Examples: . If I have the results of the US census, how much could I infer about any particular US citizen by analyzing the census? | When I look at a drug survey, can I infer that an individual participant has a certain disease? | . Contents matter! Output privacy is about giving you control over the contents you&#39;re sending to people. Unbundle what you want to share from what you don&#39;t want to share. The guarantee of output privacy can refer to any fact that can be conveyed in an information flow. . Examples: . While you are on the telephone with your boss, you might want to hide the fact that you are on a beach. | When you participate in a medical study, you might want to stay anonymous, so that your insurance company cannot learn any risk factors of you. | While on a video chat with friends, you might want to blur your background to hide the mess you&#39;re in. | . And it&#39;s not only about the information you are sharing exactly. It&#39;s also about information people might use to infer other things about you. . Example: Did you know that analytics companies scrape and parse social media, looking for products and brands in your pictures? You should be able to share what you want to say (&quot;I&#39;m enjoying life on the beach&quot;), without revealing other details. For example, the bottle of medication on the table behind you. Photo post-processing that removes or blurs sensitive items is a great example of output privacy. . Differential Privacy (DP) . Differential privacy is a sophisticated tool for output privacy. It guarantees output privacy in a specific context: aggregate statistics over a large group of people. It tries to unbundle your participation in a survey from a scientist&#39;s ability to learn patterns about people who took part in the survey. . To see how Differential Privacy works, let&#39;s construct an example. What if Andrew wanted to calculate the average age of all participants of this course. A basic way to do this: everyone sends their age to Andrew. However, this would violate input privacy. Because even though Andrew is only interested in learning the average age (the desired output of the information flow), he&#39;d be learning all the inputs in the process. But we can use input privacy like HE or SMPC to protect everyone&#39;s privacy, right? Andrew could calculate the average age without knowing the age of any particular student. It seems that output privacy just happens because input privacy is guaranteed. . But what if Andrew was really clever and sent the survey again to everyone but you. To make the math easy, let&#39;s say there are 1,000 students in the course. Previously, the average age was 27. But now, when he runs the survey again without you, the average age is 26.98. Now he could calculate from the difference that you are 20 years old. [Update: originally I said there was an error in the video. This is not the case, see this discussion on GitHub] . Another type of attack: Andrew could also pretend to be the other 999 students. When he gets the final answer, he could subtract the numbers he made up and figure out that you are 47 years old. . Note: When we homomorphically encrypt everyone&#8217;s answer, we probably get decent output privacy for free. But we don&#8217;t get a true guarantee of output privacy. Because a clever individual running the survey could learn about your personal information. Differential Privacy can turn such a scenario into a hard guarantee. No one can learn information about you from the average age. How does this work? Instead of sending your age, you first choose a random number between -100 and 100 and add it to your age. So if you&#39;re 65 years old and randomly draw the number -70, you&#39;d send Andrew the number -5. So if he reverse-engineered your input using the techniques described above, he&#39;d only learn the number -5 about you. . But doesn&#39;t this destroy the accuracy of the average age? Not if we have enough participants. Random numbers have an interesting property. If we randomly choose numbers between -100 and 100, the average of only a few numbers would be very noisy. But the more numbers we look at, the closer to 0 the average moves. If we had infinite numbers, the average would be exactly zero. . Important: If you average over enough random numbers, the randomness cancels out. So if Andrew averages over enough people in the course, the noise of the random numbers cancels out and he ends up with the same answer. But if it&#39;s exactly the same answer, wouldn&#39;t the two attacks from before still work? . The first attack: One survey with you included, and another survey with you excluded. Andrew compares the results. Before, the difference between the surveys let him infer your exact age. Now, it is your age plus your random number plus 999 people&#39;s random numbers. There is no way of telling your exact age. | The second attack: Andrew pretends to be the other 999 people. Taking the numbers from above, he could only learn -5 about you, which would tell him that you are between 0 and 95 years old - which is true for almost anybody. | Takeaway: Differential Privacy in this example allows Andrew to calculate the average age, without him being able to reverse engineer personal data. In the next section, we discuss DP in a bit more formal way. . Robust Output Privacy Infrastructure . Output privacy is actually a bit more nuanced than we covered before. It&#39;s not a binary thing - is output privacy guaranteed or not. This becomes clear when we take another look at our previous example, the average age survey. What if we didn&#39;t add numbers between -100 and 100, but between -2 and 2? That would not be great privacy protection. While Andrew wouldn&#39;t know your exact age, he would have a pretty good idea. . Output privacy is more like a degree of protection. The more randomness people add to their own data, the better the privacy protection. Differential Privacy lets us measure the difference between a strong privacy guarantee and a weak one. In our example, this is the measure between choosing numbers between -100 and 100, or between -1 and 1. In DP, this measurement is called Epsilon Îµ. . [Update 25.01.2021] Imagine Îµ like pixelating an image to hide someone&#39;s identity. The more noise we add to the image, the less Îµ goes out. . Example: A medical research center has data of 3,000 patients. It wants to let outside researchers use their data to cure cancer. How can they make sure that researchers can&#39;t reverse engineer their statistical results back to the input medical records? By giving each researcher a privacy budget. . Note: Privacy budget is a measure that says: All your aggregate results that you generate from input data must represent less than X Îµ of information. X is a measure of maximum tolerance for information reconstruction risk. . When you are not too worried that someone might reconstruct your data, you can set X to 100. If you&#39;re really worried, you can set it to 1 or even 0.1. It does not matter which specific algorithm a researcher uses, as long as it can be measured in Îµ. Îµ then provides a formal guarantee about the probability that a researcher can reconstruct your private data from their statistical results. | Every researcher gets their own Îµ. Say you had 10 researchers studying your patients. If every researcher had 20 Îµ, could they team up and leak up to 200 Îµ? In theory, they could do so if they combined their results in the right way. If you are sharing data very publicly, you should consider assigning a global Îµ for all researchers. If you are working with selected researchers unlikely to share information, an individual privacy budget is fine. | Think from the perspective of a patient. How does Îµ apply to you? Say you are at 2 hospitals and both of them share data with researchers. When both hospitals share 20 Îµ worth of information about you, that means that in total 40 Îµ could be leaked! Just because your hospital thinks they are not leaking private information about you, doesn&#39;t mean they are not. | . Example: Netflix ran an ML competition and released a dataset of users and movie ratings. The dataset was anonymized, meaning no usernames or movie titles were present. This seems like good privacy protection - had IMDb not existed. They also keep large lists of users and movie ratings. It turns out, people often rate movies on both platforms at similar points in time. Researchers from the University of Texas showed that they could de-anonymize some of the Netflix data using this approach. In the terms of DP: When Netflix released some Îµ, there was already an amount Îµ public from IMDb. Combined, it was enough Îµ that researchers could run a de-anonymization attack. . The same attacks can be run on much more sensitive data than movie reviews, for example, medical records or your browser history. The amount of Îµ running around about you measures the probability that someone can reconstruct your private information from an anonymized dataset. . Îµ is a formal upper bound on the probability that bad things can happen to you if you participate in a statistical study. Statistical study is a very broad term. It includes using everyday products like Chrome, Firefox, or an Apple product. These companies study how people use their products by collecting anonymized usage data. You are protected in all this by DP, measured by some degree of Îµ. . How much Îµ can we consider as safe? There is no single correct answer. But we can transform Îµ into a more useful number called Î², where Î² = e^Îµ. Let&#39;s say you are deciding whether to participate in a medical study. The study guarantees that no more than 2 Î² are released. What does this mean? Pick any event in your future, whether it&#39;s your insurance premiums going up or your partner leaving you. If Î² is 2, then Î² guarantees that the new probability of this event, if you participate in the study, is at most 2 times the previous probability. . Differential Privacy is constructed so that it does not care whether the event seems related. The probability that the event will happen, after you participate in the study, is no greater than Î² times the current probability. . To be continued . This lesson explored the concept of structured transparency. We took a closer look on the first two guarantees of structured transparency: Input privacy and output privacy. . Tip: The biggest changes will not come from just encrypting existing systems. They will come from new opportunities. Ask yourself: What could I never do until now, because sharing my data would be too risky? In the next summary, we&#39;ll cover the remaining guarantees: Input and output verification as well as flow governance. . If you found a paragraph that needs improvement, please let me know! I&#39;m also happy to hear from you if you found this summary helpful! ðŸ˜Š .",
            "url": "https://deeplearning.berlin/online%20courses/privacy/the%20private%20ai%20series/openmined/2021/01/19/Our-Privacy-Opportunity-Part-3.html",
            "relUrl": "/online%20courses/privacy/the%20private%20ai%20series/openmined/2021/01/19/Our-Privacy-Opportunity-Part-3.html",
            "date": " â€¢ Jan 19, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "The Private AI Series, Course 1, Part 2",
            "content": "Limitations of Information Flows . In part 1 of my summary of the Private AI series we covered lesson 2. This lesson was all about information flows and how they are fundamental to our society and human collaboration. We also learned about how information flows are often broken today because of the privacy-transparency trade-off. . Tip: To make the matter less abstract, you can replace &quot;information flows&quot; with your favorite example. Take democracy, scientific research, or communities working together to help the environment. To improve information flows, we need to understand what exactly is not working today. Not only in the general terms of privacy and transparency, but in detail. In lesson 3 we learn about three key technical problems that form the foundation for privacy and transparency issues: . The copy problem | The bundling problem | The recursive enforcement problem | The Copy Problem . . Note: The copy problem describes that you lose control over how someone uses your data once you share a copy. Suppose I have a piece of information (i.e., a document or an mp3 file). If I make a copy of this and give it to you, I lose technical control over this copy. I have to trust you that you don&#39;t use it against me, that you follow laws governing my data, that you don&#39;t share it with somebody else without my knowledge. . There are laws attempting to prevent people from misusing information, like HIPPA or GDPR or CCPA. But they are really difficult to enforce. . That&#39;s why the copy problem is so important as a technical issue. Because - no matter what the law says - it determines what people actually can do with a piece of information. . You might be tempted to say: uncontrolled copying of all information sounds terrible, let&#39;s stop this! But be careful. While the copy problem might hurt you sometimes, it is also protecting some of your most treasured freedoms. While anyone who stores your information can make copies of it, you can also copy anyone&#39;s data that you store. Any attempt to limit this ability could have a big impact on your life. . Example: Digital piracy - the sharing of copyrighted songs, movies, software - is a classic example of the copy problem. As soon as a digital copy of a file is sold to the first customer, this customer could share it with all other potential customers. There is no way for the copyright holder to control this. . In reaction to this, the entertainment industry developed DRM software. You can read about DRM in this comprehensive article. . Note: DRM stands for digital rights management. It&#8217;s a set of technologies to control the use, modification, and distribution of copyrighted works. DRM software prevents your computer from playing files you didn&#39;t buy. It is controversial because DRM software is a great potential threat to privacy and agency over consumer devices. It lets central authorities control what you can and cannot do with your personal devices. . But funding for the creation of art also stands at risk. Artists deserve compensation for the value they create! . An ideal solution would be a very selective enforcement of a copy limitation. Unfortunately, this is impossible to do: computers are machines that operate by making copies. Even a stream is a download - without a save button. But you still can make copies of the content. To prevent data from being copied, you need incredibly invasive software. . Example: Dropbox prevents you from sharing copyrighted material. They scan every file you upload to a shared folder, to check if it contains copyrighted material. . The copy problem causes a privacy-transparency trade-off. Sometimes you might want to share data, but you have to weigh the benefits of sharing against the risks of misuse. A solution would radically change many industries, offering the best that both sides have to offer. . The Bundling Problem . . Note: The bundling problem is this: It can be difficult to share a certain piece of intended information, without also needing to reveal additional information to verify the intended information. Example: A bartender checks your ID to verify your age. But he does not only see your date of birth but also your home address, your full name, where you were born et cetera. In fact, it wouldn&#39;t even be necessary for him to see your full birth date. It does not matter whether you are 19 or 49, only whether you are over 18. But if you just carried around a card that said &quot;Greater than 18&quot; or &quot;Yes&quot;, then how would the bartender verify it&#39;s true? . This problem is everywhere. More examples: . You share an image to prove something, but there are other things in that image, too | A news organization reports about protests. It shows videos of individual protesters, which could later be used against them | Researchers share sensitive medical data, when all they needed were the patterns within this data | . The Problem of Surveillance . Another example is home security systems. If you set up a video camera outside your front door, does it only record information about intruders? Of course not! It records every person that walks by, every car, every dog. Absolutely everything, 24/7 and 365 days a year. Your ability to watch the 0.01 percent of the footage that actually matters, comes bundled with the need to record also the other 99.99 percent. And we hope that the 99.99 percent are not misused. . Almost all sorts of surveillance suffer from this bundling problem. Rare events justify the collection of massive amounts of information, which is not supposed to be used for anything. Most people don&#39;t know how to build a surveillance system that only records the rare events that it is intended to identify. But at the end of this course, you will learn how to do this. . AI Governance . The bundling problem is also a topic in AI governance. . Note: AI governance is about evaluating and monitoring algorithms for effectiveness, risk, bias and ROI (Return On Investment) (Source: forbes.com) Example: Courts use AI to support them in parole decisions or sentencing decisions. Machine Learning models might predict how likely it is someone will violate paroles. Often it is hard to audit these algorithms. Do they behave like advertised? Were they developed in a responsible manner? The companies that build these algorithms have two credible reasons not do disclose any details: . How exactly the algorithm works might be valuable intellectual property | If the details of the algorithm were public, it might be easy to fool | Artificial Bundling Problems . Sometimes information that could be unbundled isn&#39;t unbundled, because someone in a powerful position does not want it to be unbundled. . Examples: . You have to provide your email address to read an article | You want to use a free trial, but you have to enter your full details and credit card | You want to text with your friends, but you have to agree that a service scans all images and links you send | . So, there are different forms of the bundling problem: . Artificial bundling problems that are forced upon you | Natural bundling problems | . The boundary between them is increasingly grey. But in this course, you will learn how to tell the difference. And in many cases, how to avoid both. . The Recursive Enforcement Problem . Couldn&#39;t third party oversight institutions solve a lot of the issues caused by the copy problem and bundling problems? Why not make undesirable uses of data illegal? While this sounds good in theory, enforcing such rules is much harder to do in practice. . Note: Recursive enforcement: when enforcing privacy regulations, we end up in a recursive loop. Each authority that supervises other entities must itself be supervised by an authority. Example: Imagine a PhD student who uses medical records for his research. We worry that he might misuse the data. We could use a third party authority to make sure nothing bad is happening. The PhD students&#39; supervisor seems like a good fit. But how would the supervisor actually detect if the student misused the data? As soon as the data is on his computer, he could do anything with it, for example share it. The supervisor is unlikely to find out. . The solution seems to be: the data must stay on the supervisor&#39;s machine, not on the student&#39;s computer. This might be a bit of an inconvenience, but now the supervisor can watch everything the student does with the data. But what about the supervisor: now he has the ability to misuse the data! Who controls the supervisor? The university? And so on. We call this the recursive enforcement problem. It is also called the recursive oversight problem. . It&#39;s one of the most important problems we face. This is the core technical problem of data governance. If you have to put data onto someone&#39;s computer, then who makes sure that that someone doesn&#39;t misuse it? . Note: Data governance is the process of managing the availability, usability, integrity and security of the data in enterprise systems [...] Effective data governance ensures that data is consistent and trustworthy and doesn&#8217;t get misused. (Source: techtarget.com) The problem of authorities needing their own authorities is also known in political science. It has been tackled through systems of decentralized governance. Things like democracy, representative government, checks and balances. . This is much harder to do with data. How can multiple people have ownership over a data point, that still has to live on a single machine? . There is a new class of technologies that allows this, and we will learn about it in the next part. . Conclusion . This lesson explored the three major technical problems that underlie the privacy-transparency trade-off. The copy problem, the bundling problem, and the recursive enforcement problem. . In the last two lessons, we learned a lot about the problems of today&#39;s information flows. In Part 3, we will begin to learn about solutions! . If you found a paragraph that needs improvement, please let me know in the comment section or on Twitter, I&#39;m @daflowjoe. I&#39;m also happy to hear from you if you found this summary helpful! :) .",
            "url": "https://deeplearning.berlin/online%20courses/privacy/the%20private%20ai%20series/openmined/2021/01/11/Our-Privacy-Opportunity-Part-2.html",
            "relUrl": "/online%20courses/privacy/the%20private%20ai%20series/openmined/2021/01/11/Our-Privacy-Opportunity-Part-2.html",
            "date": " â€¢ Jan 11, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "The Private AI Series, Course 1, Part 1",
            "content": "I started taking The Private AI Series by OpenMined . The series contains four courses, all are free. At the time of writing, only the first one is online. . Our Privacy Opportunity | Foundations of Private Computation | Federated Learning Across Enterprises | Federated Learning on Mobile | The first course is non-technical and contains about 8 hours of video, taught by Emma Bluemke and Andrew Trask. Additionally, they interview many experts. The course aims to provide an overview of what privacy means, where it currently fails, and how possible solutions look like. One key point is that there are new privacy-enhancing technologies on the rise that will change the way how humans collaborate. This brings with it many career and business opportunities. . If youâ€™re interested, sign up at courses.openmined.org! . Lesson 1 is just introductory. This is my summary of Lesson 2 of Our Privacy Opportunity. In addition, I can recommend Nahua Kang&#39;s great summary, it is more thouroughly structured than mine, which I primarily wrote for my forgetful self. . Society Runs on Information Flows . The main topic of the course is the privacy-transparency trade-off and how it affects a huge number of issues. This lesson walks through some of the most important challenges to society and identifies how the privacy-transparency trade-off underpins them. Improving information flows, by solving this trade-off, can help us in many areas like disinformation, scientific innovation, and even democracy itself. . Important: Every part of the human experience is soaked in information flows. Since the beginning of human collaboration. We share our medical information with our doctor. We share our location with an app to get directions. We share our heart rates and sleeping patterns in hopes of improving our well-being. Every day, we share personal information to exchange goods, receive services, and in general, to collaborate. Sharing information is a part of almost every aspect of our life. . Information Flow . What is an information flow? Let&#39;s take the simple example of email. A sender, a message, a receiver. Probably one of the most straight-forward information flows. But even email is much more nuanced than just the three attributes sender, message, receiver: . Should other people than the receiver be allowed to read it? | Would I be comfortable with the receiver forwarding my email? | The email provider could probably read it, do I trust them to not do so? | Do I want the email provider to read my mail only for a specific purpose, like for spam detection, but not for targeted advertising? | Am I sending my exact identity with the email? Anonymously? Or a mix: as a member of group? | Do I know exactly who the recipient is? When I&#39;m sending the mail to a doctor&#39;s office, who reads it? | Can the receiver have confidence in the identity of the sender, whaf if my account was hacked? | . Questions like these exist around every information flow. . Newly emerging communication channels: Snapchat deletes the messages once they&#39;ve been read and prohibits forwarding or screenshotting. WhatsApp or Signal use end-to-end encryption for messages so it&#39;s impossible for anyone other than the intended recipient to read them. Users switch to these services because of seemingly tiny changes to the guarantees around information flow. This is the beginning of a revolution! . Note: Definition: An information flow is a flow of bits from a sender to a receiver. The sender and receiver could either be an exact individual, a member of a group, or an anonymous individual. The identity of the sender, the receiver, and the content of the message itself can be probabilistic. The probabilistic nature is important. Often a piece of information you share does not have an exact recipient. . What Does Privacy Mean? . Privacy is not about secrecy. People feel that their privacy is violated if information flows in a way they didn&#39;t expect. It&#39;s all about the appropriate flow of information, not about the information itself. . Note: Privacy means the ability to ensure information flows that are according to social norms. Example: Google Street View: why are people having trouble with Google taking photos of them in their front yard, when anybody could come by and see them exactly there? Because when public information becomes so much more public, it bothers us. . Note: This theory of privacy is called Contextual Integrity: sharing the same information might be private in one context, but not in another context. It&#8217;s about achieving appropriate information flow. (There is a bit more to the concept, developed by Helen Nissenbaum, see details on Wikipedia) . Example: My face is considered public information as soon as I leave the house, because anybody can see it. So why is facial recognition software so troubling? Not only because it could be misused (i.e., for mass surveillance), but because it is identification without my consent. The information flow is not triggered by me, but by whatever system is watching me. . Data is Fire &#128293; . There is the popular notion that &quot;data is the new oil&quot;. A better analogy is &quot;data is fire&quot;. . It can be duplicated indefinitely | It can help us prosper and solve problems | It can cause irreparable damage if misused | . This dual-use for good or harm is true for all kinds of data, not just data that is clearly sensitive like medical data. . Everything can be private data . Your grocery shopping list is boring, right? Not always. You might not care now whether somebody knows you&#39;re buying bread. But when you suddenly stop buying bread (and other carbs), it might be an indication of the diagnosis of diabetes. Suddenly it&#39;s very private information that you might not want to share. . Careful: Anonymization doesn&#8217;t work! While anonymization seems like the obvious solution to protect the identities of people in data, this does not work reliably. Even when names are removed from data, other features can be used to identify you, thanks to the power of machine learning. . And even when your exact identity is not recoverable, data can be used for targeting: As long as someone is able to reach you (via your browser, your church, your neighborhood, ...), your name is not at all necessary to do harm. . Example: Anonymization works so badly, that systematically exploiting its weaknesses can become a business model. Emma talks about a US company that buys anonymized health data and distributes &quot;market insights&quot; from it to insurance companies. They can then, for example, avoid selling insurance to high-risk communities like poor neighborhoods, where people are more likely to get sick. . Another example: Strava released an anonymized heatmap of user activities that revealed the location of US military bases. So, privacy can be relevant not only on an individual level but on an organizational or even national security level. Strava released their global heatmap. 13 trillion GPS points from their users (turning off data sharing is an option). https://t.co/hA6jcxfBQI â€¦ It looks very pretty, but not amazing for Op-Sec. US Bases are clearly identifiable and mappable pic.twitter.com/rBgGnOzasq . &mdash; Nathan Ruser (@Nrg8000) January 27, 2018 . Privacy and Transparency Dilemmas . Remember the dual-use of data ðŸ”¥ from the previous section. Due to the potentially harmful use of data, we have to constantly make trade-offs and decide whether to share information, weighing the benefits and the risks. . Note: A privacy dilemma is a trade-off whether or not to reveal information, where revealing that information causes some social good (like advances in medical research) but could also lead to harm (like the misuse of medical data). Privacy dilemmas have various costs. The most obvious is a privacy violation where data is shared in good faith but the information is misused. On the other hand, there are societal costs when information instead is kept secret: a failure to accomplish important outcomes of information flow (scientific progress, meaningful relationships, accountability). . Tip: Privacy dilemmas are untapped market opportunities! Closely related is the transparency dilemma: . Note: A transparency dilemma is when someone is forced to make a decision without having access to the information they need to make it. Sometimes the necessary information flows don&#39;t exist at all (trusting a stranger to fix your tire), sometimes they exist but their content is not verified (online reviews). . Stopping all information flow and locking all data is not the solution to the privacy issue. This would prevent good use of data (think medical care, climate research) and also make undesirable behaviour easier (money laundering, lack of accountability). Maximizing privacy could lead to a lack of transparency! . The Privacy-Transparency Pareto Frontier . . This is the privacy-transparency trade-off. More of one means less of the other.We used to have a classic Pareto trade-off between privacy and transparency. You had to decide whether you share information at the cost of privacy (point A in the chart). Or whether you keep information private, but at the cost of transparency (B). The question is: how can we move the frontier of this trade-off and have more of both at the same time? . . With new privacy-enhancing technologies, we can have more of both privacy and transparency.With new technologies, we can actually move the pareto frontier. Notice that point B in this chart has the same amount of privacy as in the first chart, but has a lot more transparency. . We don&#39;t have a zero-sum game anymore! This will affect every industry handling valuable, sensitive, or private data. . Thanks to these technologies, in the future governments won&#39;t have to choose between preserving the privacy of their citizens or protect national security, they can do both. Researchers won&#39;t have to decide whether or not to share their data, they can have the benefits from both. Corporations currently often have to choose between the privacy of their users and the accuracy of their products and services, in the future they can have both. . How these privacy-enhancing methods look like and which specific technologies are developed, will be covered later in the course. . Why We Need to Solve the Privacy-Transparency Trade-Off . Research is Constrained by Information Flows . If there was a way to share data across institutions while making sure it remained private and was used for good, all areas of research would benefit. More data would be available, it would be available faster, and also: experiments could be replicated more easily. . Healthy Market Competition for Information Services . Most services that handle your data will profit from locking you in. Because of privacy concerns they are inherently anti-competitive. More privacy restrictions can actually make it harder for new companies to compete (because you can&#39;t move your data from your old to the new provider). . We need more interoperability between information service providers. . Note: Interoperability means you can buy your shoes from one company and your socks from another. In information services it also means that you should be able to move to a different company and take your data with you. Example: Facebook actually started as a company that profited a lot from interoperability. One reason it gained popularity was that users from its established main competitor MySpace could connect their accounts with Facebook and still message with their friends on the old platform. Without this feature, probably less people would have switched to the new platform. This is called adversarial interoperability. . Note: The GDPR (General Data Protection Regulation) was introduced in the EU in 2018 and has the aim to give individuals control over their personal data. The GDPR is considered a groundbreaking piece of legislation and it is being copied around the world. . EU citizens now have 7 rights over their data, including the right to be forgotten (a company has to delete all your personal data on request) and the right of access (on request, companies have to send you a copy of all data they have of you). . Important: Privacy is not only about preventing information from being shared. Sometimes satisfying privacy is about forcing companies to share or delete your data in a specific way or at a specific time. . Data, Energy &amp; the Environment . One of society&#39;s biggest challenges is the transition to green energy. The volatile nature of renewable energy sources makes nation-wide coordination of energy demand necessary. . An area where the privacy-transparency trade-off comes into play is smart meters. Smart meters are highly valuable for the transition to clean energy. Grid operators can have an accurate picture of energy demand, consumers can reduce energy waste. But smart meters can also be extremely privacy invasive, because one can build rich patterns of your energy data. How your daily habits are, when you are or are not at home etc. . Example: In Taiwan many people have air boxes in their homes to measure pollution. There was a community-driven effort to collect these measurements. They were able to coordinate with millions of people to get this data-sharing system working. The government didn&#39;t invest heavily in this technology, but was very interested in the data. In exchange they installed more air boxes in places like public parks and military zones. . Important: The Taiwan example shows that collaboration of millions of people is possible and can solve urgent issues. . Feedback Mechanisms &amp; Information Flows . We often rely on the opinions of others when we make our decisions. Which car do you buy, which surgeon do you choose for a surgery? But there are more feedback mechanisms. Elections, protests, Facebook likes, going to prison, boycotting, gossip, are all feedback mechanisms. . Note: Feedback mechanism: Someone does something, and later gets positive or negative feedback from those affected by their actions. Feedback mechanisms help us understand how the world views our work, so we can do more good things and fewer bad things. They are essential to society&#39;s function and unfortunately, due to the privacy-transparency trade-off, many of them are quite broken. This is the case when feedback information is too sensitive or valuable to be shared. . What does a broken feedback scenario look like? . Examples: . Medical care: When you go for surgery, how good is your surgeon? Can you ask for reviews of previous patients, could you talk to previous nurses? And even if you could, could you talk to enough patients or nurses? | Consumer products: How do you know whether a product is any good? Amazon reviews are easy to fake, and the real ones come from only the most polarized users. | Politics: A multiple choice question between a few candidates every 4 years is a terrible feedback system for reviewing the legislature of the past 4 years. | . Most feedback information simply isn&#39;t collected, because it would be too personal to collect it. . Democracy &amp; Public Health Conversation . Democracy is messy. Opinions are formed via social groups. In recent years there was an uptick in polarization, one of the reasons probably being social media where algorithms maximize engagement. . A better way can be found in Taiwan, with the Polis system. A community-built, nation-wide application that supports conversation between millions of users in Taiwan. It&#39;s not optimized for engagement, but for consensus. People can enter their opinions in written form (tweet-like), and a combination of NLP and voting clusters these opinions. Turns out, opinions aren&#39;t actually individual. There are less opinions than there are people because opinions are formed socially. However, the social groups that form our opinions aren&#39;t fixed but constantly changing. . So, some people emerge as being representative for specific opinions and become thought leaders for this particular matter. But now they must come up with a formulation that will get the most consent across opinions. . Example: When Uber wanted to come to Taiwan, people had very polarized opinions. The solution was: Uber was permitted a temporary license in Taiwan. During this time, the public Taxi sector should adopt the efficient algorithmic approaches from Uber while maintaining current labor standards. If they would succeed, Uber would be banned. If they failed, Uber would be banned unless they met the labor standards of the public system. That put just enough pressure on both sides, and in the end, the public system did improve so much that Uber was excluded. . New Market Incentives . Today&#39;s incentives of companies are often misaligned with the well-being of their users. . Example: Many online companies use attention (often called engagement) as their key metric. For some this intuitively makes sense, because their revenue is ad-driven. But even companies that run on a subscription model, like Netflix, do it. Netflix&#39;s former CEO Reed Hastings famously said they are competing with sleep (&quot;And weâ€™re winning!&quot;). The question is: why? . One answer is that it&#39;s a readily available metric which is fine-grained and allows for optimization. Netflix&#39;s number of subscribers - which is the number they actually care about - is too coarse to use as a metric. Only if a movie was so good or so bad that it made users subscribe/unsubscribe, it would have a measurable effect. . Attention as a metric does work and is probably not a problem when used at a small scale. But at large scale and taken to the extremes it can cause harm, see the Netflix/sleep example. . Let&#39;s speculate about a better approach: Netflix could try to optimize their experience to improve the users&#39; sleep. But how would they measure it and train an algorithm on it? Fitbits track sleeping patterns, but is it safe to share this data with Netflix? In general, these alternative metrics are called wellness metrics and can improve our lives. . Tip: Technology isn&#8217;t inherently addictive! Better products are possible. But we need to solve the privacy-transparency trade-off. . Safe Data Networks for Business, Governance and R&amp;D . How do privacy-transparency trade-offs affect important public information flows? . The European Commission recently proposed the Data Governance Act to improve data flows around the EU. The motivation: Businesses need data. And if they want to customize their product for each member state, they need data from these states. Data should flow easily through the EU. This increased access to data would advance scientific developments and innovations. This is especially important where coordinated action is necessary, like a global pandemic or tackling climate change. . So why should data not flow entirely freely? . Commercially sensitive data like trade secrets should be protected. Data access can lead to theft of intellectual property. | Data is valuable. Not just for a business, but for a country. Who controls the data has an impact on national security. | Data can be private or sensitive. Fundamental rights of data protection have to be respected. | New threats to privacy: New mathematical tools allow reconstruction of personal details even from anonymized datasets. Free-flowing anonymized data access only seems like a good idea if you ignore all of the European history. . Technology advances faster than legislation. Regulation has to consider the power of future analysis techniques. . The privacy trade-off here is relevant to individuals, companies and countries. Companies and users should be able to trust that their data is used in a manner that respects their rights and interests. Trust will be crucial for data to be willingly shared. . But trust doesn&#39;t just arise. How can we protect the people&#39;s rights and interests? . Let&#39;s daydream: What if the data didn&#39;t have to move? What if the institutions within the home country had the only copy of a citizen&#39;s sensitive data, which the other countries accessed remotely and easily and in a controlled manner? Instead of transferring the data around Europe, out of the owner&#39;s control? . Today, there are new techniques to enable privacy-friendly analysis, including differential privacy which will be covered during this course. . Conflict, Political Science &amp; Information Flows . One rational explanation of war: Mutual Optimism. It&#39;s extremely hard to predict the outcome of a battle, a war. Both sides can come up with an estimate that says, &quot;we&#39;re more likely to win than not to win&quot;. The sum of the estimates is greater than 1. That&#39;s why nations go to war. . A way to share private military information to determine the winner (in a digital war game) ahead of time, but without actually giving away military secrets to the opponent, could potentially avoid wars. . This is true for other conflicts as well, like legal disputes or commercial competition. If the winner could be determined ahead some conflicts wouldn&#39;t be fought. . Moving the privacy-transparency trade-off is essential here as well. . Disinformation &amp; Information Flows . The flow of news is one of the most important information flows in the world. How do you know that what you read in the news is actually true? . Before the invention of the printing press, people had the power to talk to maybe 50 people at the same time. For a story to be shared outside your own social circle, you would have to convince other people to talk about it. But today, where the average person has hundreds of contacts on social media, fake news and rumors can spread easily. . How to check if news is true? . Have social media platforms emply people who check every bit that is published? Not feasible for hundreds of millions of users. | Let a machine learning algorithm check whether a piece of news is true? Probably a bad idea in the long run, because news are an information bottleneck. Detecting fake news only by reading it doesn&#39;t work, you have to have knowledge of the world. | Just get off social media? Maybe we&#39;re not supposed to be interconnected with that many people? | . The most interesting solution is currently being deployed in Taiwan: . The Polis platform (developed by a hacker collective called g0v, pronounced &quot;gov zero&quot;) aims to improve public discourse. Trained volunteers comment on suspicious stories with reliable sources one might check. Since these comments come from people you know from your local community, you already have a higher level of trust to them. . Important: We have to consider how societies historically dealt with misinformation. It doesn&#39;t fix the problem to let the platforms take down false posts. People are curious and won&#39;t just accept this as &quot;huh, this is false then&quot;. This is the beauty of Audrey Tang&#39;s work with g0v: constructing information flows that are healthy for society. Not thinking about the most efficient way to prevent a data flow. But to activate existing ways to fight disinformation: get people to help their friends. This might not seem as efficient, but will be more effective in the long run. . Another approach in Taiwan: using humor to foster trust between the state and its citizens. Humor over rumor! ðŸ‡¹ðŸ‡¼ #Taiwan is combating #Coronavirus &amp; managing the #COVID19 pandemic.ðŸ’¡ Digital Social Innovation is key!ðŸš€ Itâ€™s fast, open, fair &amp; fun.ðŸ™Œ Most importantly, it needs #AllHandsOnDeck.ðŸ•” Take 5 with me &amp; get up to speed.ðŸ’» Visit https://t.co/5D68ia7PcI &amp; learn much more. pic.twitter.com/M5ecPnSPLF . &mdash; Audrey Tang å”é³³ (@audreyt) April 21, 2020 . Conclusion . The privacy-transparency trade-off or even privacy in general is in service of a higher aim: creating information flows within society that create social good. . Important: Privacy technology is not just about more privacy. Don&#39;t just look for use cases that scream &quot;privacy&quot;. Instead, ask yourself: How can society accomplish its goals with less risk, higher accuracy, faster, and with better aligned incentives than ever before, through better flows of information. . Tip: Entrepreneurial opportunities, regulatory opportunities, investing opportunities: It&#8217;s not about hiding data; it&#8217;s about enabling specific information flows (and just these!) to maximize social good. That is the promise of privacy-enhancing technology and has the potential to radically improve every aspect of how we share information. . I hope you found this summary helpful! Please let me know any feedback you have here in the comments or on Twitter, I&#39;m @daflowjoe. . In Part 2 we will learn about the technical problems that cause the privacy-transparency trade-off. .",
            "url": "https://deeplearning.berlin/online%20courses/privacy/the%20private%20ai%20series/openmined/2021/01/08/Our-Privacy-Opportunity-Part-1.html",
            "relUrl": "/online%20courses/privacy/the%20private%20ai%20series/openmined/2021/01/08/Our-Privacy-Opportunity-Part-1.html",
            "date": " â€¢ Jan 8, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Understanding fastai's Midlevel API",
            "content": "Even after going through the course and book, I didn&#39;t feel comfortable with the numerous classes and methods that fastai offers for data processing. I felt that it would be easy once I understood it, but it just didn&#39;t click right away. So here I try to present the most important classes and why they are useful. . This is by no means a complete overview, but just an exploration with minimal examples. . For a deeper dive, please check out chapter 11 of fastbook, Wayde Gilliam&#39;s awesome blog post, or Zach Mueller&#39;s walk with fastai2. . Transforms . A Transform in general is an object that can be called (behaves like a function) and has an optional setup method that initializes an inner state, and an optional decode method that will reverse the function. . In general, our data is always in tuples of (input,target), although you can have more than one input or one target. When applying a transform, it should be applied to the elements of the tuple separately. . . . Note: Transforms are usually applied on tuples. . When you only want to implement the encoding behaviour of a transform, it can be defined via the @Transform decorator: . @Transform def lowercase(x:str): return str.lower(x) lowercase(&quot;Hello, Dear Reader&quot;) . &#39;hello, dear reader&#39; . Type dispatch . Transforms can be defined so they apply only to certain types. This concept is called type dispatch and is provided by fastcore, which I&#39;ll cover in a future post :-) . @Transform def square(x:int): return x**2 square((3, 3.)) . (9, 3.0) . Notice the type annotation for x. In this case, this is not merely a helpful annotation like in pure Python, but it actually changes the behaviour of the function! The square transform is only applied to elements of the type int. When we don&#39;t define the type, a transform is applied on all types. . A more complex transform . If you want to also define the setup and decode methods, you can inherit from Transform: . class DumbTokenizer(Transform): def setups(self, items): vocab = set([char for text in items for char in text]) self.c2i = {char: i for i, char in enumerate(vocab)} self.i2c = {i: char for i, char in enumerate(vocab)} def encodes(self, x): return [self.c2i.get(char, 999) for char in x] def decodes(self, x): return &#39;&#39;.join([self.i2c.get(n, &#39; ? &#39;) for n in x]) . texts = [&quot;Hello&quot;, &quot;Bonjour&quot;, &quot;Guten Tag&quot;, &quot;Konnichiwa&quot;] tokenizer = DumbTokenizer() tokenizer.setup(texts) encoded = tokenizer(&quot;Hello!&quot;) encoded . [9, 6, 2, 2, 4, 999] . Now this is a representation that a machine learning model can work with. But we humans can&#39;t read it anymore. To display the data and be able to analyze it, call decode on the result: . tokenizer.decode(encoded) . &#39;Hello ? &#39; . So here we defined a (very dumb) tokenizer. The setups method receives a bunch of items that we pass in. It creates a vocabulary of all characters that appear in items. In encodes we transform each character to a number in an index. When the character is not found in the vocabulary, it is replaced with a 999 token. decodes reverses this transform as good as it can. . Notice the ? in the decoded representation instead of !. Since there was no ! in the initial texts, the tokenizer replaced it with the token for &quot;unkown&quot;, 999. This is then replaced with ? during decoding. . By the way: you might have noticed that in the DumbTokenizer class we defined the methods setups, encodes and decodes, but on the instance tokenizer we call methods with slightly different names (setup, decode) or even the instance directly: tokenizer(...). The reason for this is that fastai applies some magic in the background, for example it checks that the type is not changed by the transforms. . Pipeline . A Pipeline is just an easy way to apply a list of transforms, in order. . tfms = Pipeline([lowercase, tokenizer]) encoded = tfms(&quot;Hello World!&quot;) encoded . [17, 6, 2, 2, 4, 11, 5, 4, 0, 2, 999, 999] . Pipeline also supports decoding of an item: . tfms.decode(encoded) . &#39;hello worl ? ? &#39; . Because we didn&#39;t define a decodes method for lowercase, this transform cannot be reversed. The decoded result consists only of lowercase letters. . What Pipeline doesn&#39;t provide is support for the setup of the transforms. When you want to apply a pipeline of transforms on a list of data, TfmdLists comes to the rescue. . TfmdLists . At first, your data is usually a set of raw items (like filenames or rows in a dataframe) to which you want to apply some transforms. To combine your pipeline of transforms with your set of raw items, use TfmdLists. . texts = [&quot;Hello&quot;, &quot;Bonjour&quot;, &quot;Guten Tag&quot;, &quot;Konnichiwa&quot;] tls = TfmdLists(texts, [DumbTokenizer]) . When initialized, TfmdLists will call the setup method of each Transform in order, providing it with all the items of the previous Transform. To get the result of the pipeline on any raw element, just index into the TfmdLists: . encoded = tls[1] encoded . [8, 4, 14, 15, 4, 19, 0] . tls.decode(encoded) . &#39;Bonjour&#39; . Training and validation sets . The reason that TfmdLists is named with an s (lists in plural) is that it can handle a training and a validation set. Use the splits argument to pass . the indices of the elements that should be in the training set | the indices of the elements that should be in the validation set. | . We will just do this by hand in our toy example. The training set will be &quot;Hello&quot; and &quot;Guten Tag&quot;, the other two go in the validation set. . texts = [&quot;Hello&quot;, &quot;Bonjour&quot;, &quot;Guten Tag&quot;, &quot;Konnichiwa&quot;] splits = [[0,2],[1,3]] tls = TfmdLists(texts, [lowercase, DumbTokenizer], splits=splits) . We can then access the sets through the train and valid attributes: . encoded = tls.train[1] encoded, tls.decode(encoded) . ([3, 7, 1, 6, 4, 8, 1, 0, 3], &#39;guten tag&#39;) . Let&#39;s look at at word in the validation set: . encoded = tls.valid[0] encoded, tls.decode(encoded) . ([999, 2, 4, 999, 2, 7, 999], &#39; ? on ? ou ? &#39;) . Ouch, what happened to our &quot;Bonjour&quot; here? When TfmdLists automatically called the setup method of the transforms, it only used the items of the training set. Since there was no b, j or r in our training data, the tokenizer treats them as unknown characters. . . Important: The setup methods of the transforms receive only the items of the training set. . Don&#39;t we need labels? . Maybe you noticed that we haven&#39;t dealt with tuples until now. We only have transformed our input, we don&#39;t have a target yet. . TfmdLists is useful when you built a custom Transform that performs data preprocessing and returns tuples of inputs and targets. You can apply further transforms if you want, and then create a DataLoaders object with the dataloaders method. . . Usually however, you will have two parallel pipelines of transforms - one to convert your raw items to inputs, and one to convert raw items to targets (ie labels). To do this we can use Datasets. . Datasets . To complete our quick tour through fastai&#39;s midlevel API, we look at the Datasets class. It applies two (or more) pipelines in parallel to a list of raw items and builds tuples of the results. It performs very much like a TfmdLists object, in that it . automatically does the setup of all Transforms | supports training and validation sets | supports indexing | . The main difference is: When we index into it, it returns a tuple with the results of each pipeline. . Let&#39;s look at how to use it. For this toy example, let&#39;s pretend we want to classify whether a text contains at least one space (that&#39;s a dumb example, I know). For this we create a little labelling function in the form of a Transform. . class SpaceLabeller(Transform): def encodes(self, x): return int(&#39; &#39; in x) def decodes(self, x): return bool(x) . x_tfms = [lowercase, DumbTokenizer] y_tfms = [SpaceLabeller] dsets = Datasets(texts, [x_tfms, y_tfms], splits=splits) item = dsets.valid[1] item . ([10, 14, 13, 13, 8, 2, 7, 8, 22, 0], 0) . dsets.decode(item) . (&#39;konnichiwa&#39;, False) . At last, we can create a DataLoaders object from the Datasets. To enable processing on the GPU, two small tweaks are required. First, every item has to be converted to a tensor (often this will happen earlier, as one of the transforms in the pipeline). Second, we use a fastai function called pad_input to make every sequence the same length, since a tensor requires regular shape. . dls = dsets.dataloaders(bs=1, after_item=tensor, before_batch=pad_input) . dls.train_ds . (#2) [([7, 4, 11, 11, 14], 0),([6, 20, 19, 4, 13, 999, 19, 0, 6], 1)] . This is now a ready-for-training dataloader! . Conclusion . We looked at how to customize every step of the data transformation pipeline. As mentioned in the beginning, this is not a complete description of all the features available, but a good starting point for experimentation. I hope this was helpful to you, let me know if you have questions or suggestions for improvement at hannes@deeplearning.berlin or @daflowjoe on Twitter. .",
            "url": "https://deeplearning.berlin/fastai/data%20processing/2020/12/16/Understanding-fastai-midlevel-API.html",
            "relUrl": "/fastai/data%20processing/2020/12/16/Understanding-fastai-midlevel-API.html",
            "date": " â€¢ Dec 16, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Getting started with fast.ai",
            "content": "Welcome! . Summary: I cover some questions you might have regarding the fast.ai course. Then, I offer some advice that would have helped me in the beginning. . Who is this article for? . If you&#39;re reading this article, I assume you have heard about the terms deep learning, machine learning, or artificial intelligence. Maybe you are not too sure what they mean specifically, but you want to know more. Maybe you&#39;ve been searching for a while to finally learn about these concepts, but are overwhelmed by the magnitude of courses, books and software available. . Whether you are a software developer planning to add a new skillset, a student wanting to broaden your horizon, or an expert in a completely different field and you want to apply AI in your company or project: The fast.ai online course can be great for you to get started, and it&#39;s completely free! . . About fast.ai . fast.ai is an organization committed to making deep learning more accessible. It was founded by Jeremy Howard and Rachel Thomas, both distinguished data scientists. . They offer four major elements that can help you get into deep learning. . fastai (notice the missing dot), a software library that allows creating powerful deep learning models with few lines of code, while nonetheless being very customizable. | A free online course based on the library. | A book that is designed to go hand in hand with the course. It&#39;s available as a paper book, as an e-book or even for free on Github. | A forum with a very helpful community. | . For me, what really sets fast.ai apart from other online courses, is their practical approach: . We [are] always teaching through examples. We ensure that there is a context and a purpose that you can understand intuitively, rather than starting with algebraic symbol manipulation. . This does not mean that the foundations are not covered (they are!), but the order is different than in other books or courses, where you start with basic tools and only get to usable applications at the end of the course - if ever. . What is covered in the course? . The course covers major applications of deep learning. There is a certain focus on computer vision, but the other topics like tabular data, natural language processing (NLP), and recommender systems are explained as well. You will be able to create very well performing models in all of these areas, and as early as lecture 2 you can create a working web app that can recognize grizzly bears and brown bears (or anything else you choose). . Later in the course, you will learn the foundations of deep learning. You&#39;ll write code for stochastic gradient descent and activation functions from scratch. Don&#39;t worry if you never have heard about these, it&#39;s explained very well. You will dig deeper into PyTorch, the underlying software of fastai. . Also, there is a lecture (and a chapter) on Data Ethics. It is taught by Rachel Thomas and will give you a lot of food for thought. I think it&#39;s great that fastai encourages you to think about the possible implications of your work early on. . Should I get the book? . Since the book is freely available online in the form of notebooks, you might be wondering if you should get the printed book anyway. My opinion: If you can afford it - yes, you absolutely should get it! Its layout is beautiful, which makes it easier to read than the online version. Also, I personally just like to read a physical copy: my attention span is longer and I tend to take it more seriously. . That being said, you certainly can work with the free online version. It has the added benefit that you will always have the latest version, which is up to date and where bugs are fixed (and there are a few in the book). . . Do I need to know Python? . Yes - or any other language. The book is called &quot;Deep Learning for Coders&quot;, so it will not explain every line of code in detail or teach Python from scratch. In fact, the course website states: . The only prerequisite is that you know how to code (a year of experience is enough), preferably in Python, and that you have at least followed a high school math course. . However, you don&#39;t have to be an expert. When I first started with the course, I only had a few weeks of experience with Python. I did however have some (very basic) experience with web languages like JavaScript and PHP, that helped me pick up Python pretty quickly. If you know any programming language, you won&#39;t have a problem, since Python is rather accessible and has an easy syntax. If you don&#39;t have any programming experience, I recommend investing a few weeks to learn the basics of Python before you tackle fastai. There are wonderful tutorials available for free. . . I&#39;m in! How do I begin? . With lesson 1! ðŸ˜Š You really can get started right away, the interactive content runs on ready-to-use and free platforms like Google Colab, so you don&#39;t have to spend time setting up your own machine. . The course website gives you an overview of the course (you can read it in addition to this article), then you can start with lesson 1. . I enjoyed combining the videos and the book. After every lesson, I ran the associated notebooks on Colab and then read the chapters in the book. The chapters are very similar to the video lessons, but I think it helps consuming the content in a different medium and being able to go back a few pages if you want to read something again. . My suggestions for efficient learning . Watch the videos twice . I recommend watching the videos twice. On the first view, don&#39;t focus on the details too much, just get an overview of the topics covered and take some notes while doing so. This is especially relevant for the later lessons that contain much more code. On the second view, you can take more detailed notes and try to get all the details. . Take notes! . Taking notes was a gamechanger for me. I think it&#39;s the best way to stay active during the videos, and over time the notes will serve as a central knowledge repository. Also, it&#39;s great to have a place to jot down your questions, so you can try and answer them later! . I keep notes in Microsoft OneNote, you can of course use any other application or paper. I created a template based on this awesome forum post. The template contains following points: . Key Points from the lecture | Advice from Jeremy | To-Do challenges from the further research section of the chapter, ideas for projects | Reading &amp; Exploring papers that are mentioned, stuff you find on the web but don&#39;t have time for at the moment | Questions that arise during the lecture | as the last point, I copy &amp; paste the questionnaire. | . Please read the above-mentioned forum post for details on each section, it&#39;s a really good system. . These notes are meant to be used not just once, but you should refine them and work with them continuously. They can serve you as your go-to resource every time you study. All your open questions, your project ideas, and of course lots of knowledge can live there. . . Don&#39;t get stuck on one thing that you don&#39;t understand . This is an important point and I think Jeremy mentions it as well. You don&#39;t have to understand every detail right away. It&#39;s often better to move on and revisit the part you didn&#39;t quite get later. . Run the code . Jeremy will ask you to do this, and you really should run the notebooks for yourself. I recommend the clean versions, where there is no text, just code. Predict what the output of a cell will be, and if you were wrong, go back and understand why. Just reading the code in the book is not enough, it can give you the illusion that you understood it, when in reality you could not reproduce it. I fell into this trap more than once... . Do the questionnaire . Take the time and answer every question in the questionnaire. This might take a while, but it makes sure that you really understood all important concepts. You can find answers to most questions in the forum. . Additionally, I can recommend aiquizzes.com, where many questions and their answers are made available together with the relevant part of the lecture. You can use this for spaced repetition learning, the website will remind you when older questions need reviewing, it&#39;s great. . Use other sources . Fastai, while being excellent, is not the only source of wisdom. I often google for concepts I don&#39;t quite get. More often than not I find a blog post, a stack overflow answer or a video explaining it very well or with a different approach than Jeremy. . What to do after the course . The course covers only around half the book. There will be a part 2 that covers the more advanced chapters, but as of now (December 2020) there is no release date announced. If you want to dig deeper into the material, you should not wait for the course but work through the book on your own. . In addition, I&#39;d suggest picking a project and make it as good as you can. Go on the forums and see if you can help others, or ask for advice. Write about your journey in your own blog - like the one whose first post you are reading here ðŸ˜Š . Thank you for reading . Since you made it this far, I hope you found this article interesting and I could get you excited for fast.ai! I really can recommend taking the course and reading the book, and if you put in the time you will see amazing results soon. Let me know if you have any questions or found this article helpful. I&#39;m on Twitter, I&#39;d be happy to hear from you there or via mail at hannes@deeplearning.berlin .",
            "url": "https://deeplearning.berlin/fastai/getting%20started/online%20courses/2020/12/09/Getting-started-with-fastai.html",
            "relUrl": "/fastai/getting%20started/online%20courses/2020/12/09/Getting-started-with-fastai.html",
            "date": " â€¢ Dec 9, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Johannes Stutz. I am learning and writing about how machines can learn. My interest in all things AI began with my fascination with Teslaâ€™s Autopilot software. Would it actually be possible for a car to drive itself in every situation? . I wanted to learn more about the technology behind this and have embarked on a journey into machine learning. Since I donâ€™t have a professional background in coding or computer science, I go with â€œlearning by doingâ€. This approach is especially endorsed by fast.aiâ€™s great online courses. . I work as a pilot for a large airline. I think digitalization and AI can help aviation become more sustainable and even safer. But from my experience with partially automated systems, I also know how important it is to keep the human in the loop. AI should not replace humans, it should support them! . Reach out to me on Twitter, I enjoy talking and learning about machine learning, aviation, space flightâ€¦ letâ€™s nerd out! . . Thank you . This site is built with fastpages, An easy to use blogging platform with extra features for Jupyter Notebooks. . Thank you to wal_172619 on Pixabay for the picture of Berlin TV tower. .",
          "url": "https://deeplearning.berlin/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://deeplearning.berlin/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}